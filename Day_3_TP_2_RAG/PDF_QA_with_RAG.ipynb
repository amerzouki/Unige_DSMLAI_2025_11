{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "018d76a9",
   "metadata": {},
   "source": [
    "\n",
    "# Building a PDF Question Answering (QA) System using RAG\n",
    "> In this notebook, you will create a system that answers questions based on the content of a PDF file. \n",
    "> This involves building a Retrieval-Augmented Generation (RAG) pipeline, where relevant document sections \n",
    "> are retrieved based on the question and used to generate precise answers.\n",
    "\n",
    "## Key Components:\n",
    "1. **Document Loading** - Load and preprocess PDF content for analysis.\n",
    "2. **Indexing** - Split and store text in a vectorized format for fast retrieval.\n",
    "3. **Retrieval** - Retrieve relevant document sections for a given query.\n",
    "4. **Generation** - Use a language model to generate an answer based on the retrieved context.\n",
    "\n",
    "Let's dive in!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea7bd9e",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ“¥ Library Installation\n",
    "We install the necessary libraries to process PDFs, handle embeddings, and work with language models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU pypdf langchain_community\n",
    "%pip install faiss-cpu\n",
    "%pip install langchain-openai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“¤ Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import config  # Importing the config file\n",
    "import faiss\n",
    "import numpy as np\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_openai import OpenAIEmbeddings, OpenAI\n",
    "from langchain_community.vectorstores import FAISS # A library for efficient similarity search and clustering of dense vectors.\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "## Loading Documents\n",
    "> First, you'll need to choose a PDF to load. Feel free to use a PDF of your choosing.\n",
    "> Once you've chosen your PDF, the next step is to load it into a format that an LLM can more easily handle, since LLMs generally require text inputs. \n",
    "> LangChain has a few different built-in PDF document loaders for this purpose which you can experiment with. \n",
    "\n",
    "> Below, we'll use one powered by the <a href=\"https://pypi.org/project/pypdf/\">pypdf</a> package that reads from a filepath.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Documents\n",
    "\n",
    "# Example document\n",
    "file_path = \"./example_data/journal.pone.0264429.pdf\"\n",
    "\n",
    "# The loader reads the PDF at the specified path into memory.\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "# Extract text data using the pypdf package.\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore loaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type and length of docs\n",
    "print(type(docs))\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What do each element of the variable docs represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type of docs elements\n",
    "print(f\"Type of docs elements: \\n{type(docs[0])}\")\n",
    "# Display first element of docs\n",
    "print(f\"\\nFirst element of loaded data docs: \\n{docs[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So what just happened?**\n",
    "\n",
    "- The loader reads the PDF at the specified path into memory.\n",
    "- It then extracts text data using the pypdf package.\n",
    "- Finally, it creates a LangChain Document for each page of the PDF with the page's content and some metadata about where in the document the text came from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Splitting\n",
    "We split the document into smaller (more focused) chunks that can more easily fit into an LLM's context window. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Splitting\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore results of text splitting into chunks\n",
    "print(f\"Number of chuncks: {len(documents)}\\n\")\n",
    "print(f\"Type of chuncks: {type(documents[0])}\\n\")\n",
    "print(f\"Content of first text chunck:\\n{documents[1].page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Question**: Try changing the chunk sizes! \n",
    "- How does it affect the text splitting result?\n",
    "- How does it affect the generated answers below?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing\n",
    "## Embedding and Vector Storage\n",
    "Once we have split text chunks, we store them as vectors (embeddings) into a Vector Store. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Set your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = config.OPENAI_API_KEY\n",
    "\n",
    "vectorstore= InMemoryVectorStore.from_documents(\n",
    "    documents=documents, embedding=OpenAIEmbeddings()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval\n",
    "We set up a retriever from the vector store to identify relevant document chunks based on a user's query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Change number of retrieved documents\n",
    "# CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retriever allows us to perform similarity search, and retrieve relevant text chunks based on query embeddings.\n",
    "\n",
    "This involves comparing query embeddings with stored text embeddings to retrieve the most relevant passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query examples\n",
    "query = \"What is the goal of the study?\"\n",
    "#query = \"What is 95-95-95?\" #Check retrieved text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "print(f\"Number of retrieved text chuncks: {len(retrieved_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What is the default number of retrieved text chunks?\n",
    "- Change the number of retrieved chunks, e.g. 1. See [Documentation](https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/vectorstore/#specifying-top-k)\n",
    "- See effect on LLM answer below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Generation with OpenAI\n",
    "Here, we use the retrieved document chunks to answer the user query through an OpenAI language model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Set your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = config.OPENAI_API_KEY\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation (Context Integration) Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retrieved text provides context, enabling the model to generate accurate and contextually relevant responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains import create_retrieval_chain\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Keep the \"\n",
    "    \"answer precise and concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chain for passing a list of Documents to a model.\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "# Create retrieval chain that retrieves documents and then passes them on.\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "results = rag_chain.invoke({\"input\": query})\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(f\"Answer: {results['answer']}\\n\")\n",
    "print(\"Sources:\")\n",
    "for document in results[\"context\"]:\n",
    "    print(f\"\\n {document}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Change the number of retrieved text chunks used as context and play with the augmented prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“ Summary\n",
    "In this notebook, you built a PDF ingestion and question-answering system using Retrieval-Augmented Generation (RAG).\n",
    "\n",
    "You covered the following aspects:\n",
    "> - Loading and processing a PDF file into manageable text chunks\n",
    "> - Vectorizing and storing the document chunks in a vector store for similarity search\n",
    "> - Setting up a retriever to find relevant sections based on a userâ€™s query\n",
    "> - Integrating with OpenAI to generate contextually relevant responses based on the retrieved information\n",
    "\n",
    "\n",
    "\n",
    "# ðŸ”— References:\n",
    "- [LangChain - PDF Q/A](https://python.langchain.com/docs/tutorials/pdf_qa/)\n",
    "- [LangChain Tutorials](https://python.langchain.com/docs/tutorials/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv314 (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
